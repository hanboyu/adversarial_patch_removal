{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Removal Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bill_\\AppData\\Local\\Temp\\ipykernel_16984\\13163418.py:11: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import rotate\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from train_main import CIFAR10Classifier\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from AddPatch import AddPatch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR 10 Dataset with Adversarial Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    AddPatch(32, 0.05)\n",
    "    ])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img, out_path):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    npimg = np.transpose(npimg * 255, (1, 2, 0))\n",
    "    npimg = npimg.astype(\"uint8\")\n",
    "    # plt.imshow(npimg)\n",
    "    img = Image.fromarray(npimg, \"RGB\")\n",
    "    img.save(out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images), \"./plots/original_images.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bill_\\anaconda3\\envs\\cv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:899: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CIFAR10Classifier(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (train_accuracy): MulticlassAccuracy()\n",
       "  (val_accuracy): MulticlassAccuracy()\n",
       "  (test_accuracy): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load model\n",
    "model = CIFAR10Classifier.load_from_checkpoint(\"experiment_results/resnet18-2022-12-08-17-10/version_0/checkpoints/epoch=49-step=54700.ckpt\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models Accuracy Without Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 tensor(1440, device='cuda:0') tensor(0.1440, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "# Here, we use enumerate(training_loader) instead of\n",
    "# iter(training_loader) so that we can track the batch\n",
    "# index and do some intra-epoch reporting\n",
    "for i, data in enumerate(testloader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    logits = model(inputs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    correct = preds == labels[0]\n",
    "\n",
    "    total_count += len(labels)\n",
    "    correct_count += torch.sum(correct)\n",
    "\n",
    "\n",
    "print(total_count, correct_count, correct_count / total_count)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Patch Removal Defense  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 tensor(7116) tensor(0.7116)\n",
      "10000 tensor(7136) tensor(0.7136)\n",
      "10000 tensor(7126) tensor(0.7126)\n",
      "10000 tensor(7193) tensor(0.7193)\n",
      "10000 tensor(7201) tensor(0.7201)\n",
      "10000 tensor(7224) tensor(0.7224)\n",
      "10000 tensor(7281) tensor(0.7281)\n",
      "10000 tensor(7273) tensor(0.7273)\n",
      "10000 tensor(7312) tensor(0.7312)\n",
      "10000 tensor(7350) tensor(0.7350)\n"
     ]
    }
   ],
   "source": [
    "from local_gradients_smoothing.configs.configure import Configuration\n",
    "from local_gradients_smoothing.lgs.local_gradients_smoothing import LocalGradientsSmoothing\n",
    "\n",
    "cfg = {'smoothing_factor': 2.3,\n",
    "        'window_size': 5,\n",
    "        'overlap': 1,\n",
    "        'threshold': 0.1,\n",
    "        'grad_method': \"Gradient\"\n",
    "        }\n",
    "\n",
    "loc_grad_smooth = LocalGradientsSmoothing(**cfg)\n",
    "\n",
    "# threshold = 0.5\n",
    "\n",
    "acc_list = []\n",
    "threshold_list = np.linspace(0.9, 1, num=10)\n",
    "\n",
    "for threshold in threshold_list:\n",
    "\n",
    "    # show images\n",
    "    # dataiter = iter(testloader)\n",
    "    # images, labels = next(dataiter)\n",
    "\n",
    "    # masks = torch.zeros_like(images)\n",
    "\n",
    "    # for i in range(len(images)):\n",
    "    #     grad_mask = loc_grad_smooth(images[i]).squeeze(0)\n",
    "\n",
    "    #     grad_mask[grad_mask >= threshold] = 1\n",
    "    #     grad_mask[grad_mask < threshold] = 0\n",
    "        \n",
    "    #     grad_mask = grad_mask.repeat((3, 1, 1))\n",
    "        \n",
    "    #     images[i] = images[i] * (1 - grad_mask)\n",
    "    #     masks[i] = grad_mask\n",
    "\n",
    "    # imshow(torchvision.utils.make_grid(images), \"./plots/filtered_images\" + str(int(threshold)).replace(\".\", \"_\") + \".png\")\n",
    "    # imshow(torchvision.utils.make_grid(masks), \"./plots/grad_mask_images\" + str(int(threshold)).replace(\".\", \"_\") + \".png\")\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    with torch.no_grad(): \n",
    "        for i, data in enumerate(testloader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            for j in range(len(inputs)):\n",
    "                \n",
    "                grad_mask = loc_grad_smooth(inputs[j]).squeeze(0)\n",
    "\n",
    "                grad_mask[grad_mask >= threshold] = 1\n",
    "                grad_mask[grad_mask < threshold] = 0\n",
    "                \n",
    "                grad_mask = grad_mask.repeat((3, 1, 1))\n",
    "                \n",
    "                inputs[j] = inputs[j] * (1 - grad_mask)\n",
    "\n",
    "                # # img_t = transforms.ToTensor()(inputs)\n",
    "                # collage_t = torch.cat([inputs[0], grad_mask, inputs[0] * (1 - grad_mask)], dim=-1)\n",
    "                # collage = transforms.ToPILImage()(collage_t)\n",
    "                # collage.show()\n",
    "                # result_path = cfg.get('TESTING')['result_path']\n",
    "                # collage.save(result_path)\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs).to(\"cpu\")\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct = preds == labels\n",
    "\n",
    "            \n",
    "            total_count += len(labels)\n",
    "            correct_count += torch.sum(correct)\n",
    "\n",
    "        print(total_count, correct_count, correct_count / total_count)\n",
    "        acc_list.append(float(correct_count / total_count))\n",
    "\n",
    "np.savez(\"./acc.npz\", threshold_list, acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1        0.19769999 0.2141     0.23890001 0.26390001 0.28479999\n",
      " 0.30610001 0.32609999 0.3497     0.36489999]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npzfile = np.load(\"./acc.npz\")\n",
    "print(npzfile[\"arr_1\"])\n",
    "\n",
    "plt.plot(npzfile[\"arr_0\"], npzfile[\"arr_1\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "844d117fd4e1afe49933a29a2115c43b8e1468293e99afd0f20f8108dbb7378d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
